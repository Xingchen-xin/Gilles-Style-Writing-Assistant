# GSWA Environment Configuration
# Copy to .env and customize

# === SECURITY (DO NOT CHANGE) ===
ALLOW_EXTERNAL_API=false

# === LLM Backend ===
# Supported backends:
#   - vllm: For Linux servers with NVIDIA GPU (default)
#   - ollama: For Mac (Apple Silicon M1/M2/M3) or Linux
#   - lm-studio: For desktop users with LM Studio app
#
# Choose your backend:
LLM_BACKEND=vllm

# === LLM Server ===
# Default URLs by backend (auto-configured if not set):
#   - vllm: http://localhost:8000/v1
#   - ollama: http://localhost:11434/v1
#   - lm-studio: http://localhost:1234/v1
#
# Override if using non-default port:
# VLLM_BASE_URL=http://localhost:8000/v1

# Model name (adjust based on your backend):
#   - vllm: mistral-7b-instruct, llama-2-7b-chat, etc.
#   - ollama: mistral, llama2, codellama, etc.
#   - lm-studio: depends on loaded model
VLLM_MODEL_NAME=mistral

# API key (usually not needed for local servers)
VLLM_API_KEY=dummy

# === Similarity Thresholds ===
THRESHOLD_NGRAM_MAX_MATCH=12
THRESHOLD_NGRAM_OVERLAP=0.15
THRESHOLD_EMBED_TOP1=0.88

# === Generation ===
DEFAULT_N_VARIANTS=3
MAX_N_VARIANTS=5
TEMPERATURE_BASE=0.3
TEMPERATURE_VARIANCE=0.15
MAX_NEW_TOKENS=1024

# === Paths ===
CORPUS_PATH=./data/corpus/parsed
INDEX_PATH=./data/index
LOG_PATH=./logs

# === Embedding ===
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
