{
  "_description": "Training profiles for different hardware configurations",
  "_instructions": [
    "系统会自动检测你的硬件并选择最佳配置 / System auto-detects your hardware",
    "你也可以手动设置 'active_profile' 来指定配置 / Or set 'active_profile' manually",
    "Mac: MLX backend / Linux & Windows: LoRA backend",
    "",
    "内存不足解决方案 / OOM Solutions:",
    "  1. 使用 --memory-safe 标志 / Use --memory-safe flag",
    "  2. 减少 batch_size 到 1 / Reduce batch_size to 1",
    "  3. 减少 max_seq_length (512/1024) / Reduce max_seq_length",
    "  4. 减少 num_layers (4/8) / Reduce num_layers",
    "  5. 使用更小的模型如 phi3 / Use smaller model like phi3"
  ],

  "auto_detect": true,
  "active_profile": null,
  "memory_safe_mode": true,

  "recommended_models": {
    "_note": "推荐的基底模型 / Recommended base models for scientific writing",
    "for_8gb_vram": {
      "model": "Qwen/Qwen2.5-1.5B-Instruct",
      "mlx": "mlx-community/Qwen2.5-1.5B-Instruct-4bit",
      "note": "最小模型，适合低显存 / Smallest model for low VRAM"
    },
    "for_16gb_vram": {
      "model": "Qwen/Qwen2.5-7B-Instruct",
      "mlx": "mlx-community/Qwen2.5-7B-Instruct-4bit",
      "alt": "mistralai/Mistral-7B-Instruct-v0.3",
      "note": "推荐大多数用户 / Recommended for most users"
    },
    "for_24gb_vram": {
      "model": "Qwen/Qwen2.5-14B-Instruct",
      "mlx": "mlx-community/Qwen2.5-14B-Instruct-4bit",
      "note": "更好的质量 / Better quality"
    },
    "for_48gb_plus": {
      "model": "mistralai/Mistral-Large-Instruct-2407",
      "note": "最佳质量 / Best quality"
    }
  },

  "profiles": {
    "mac_m1_8gb": {
      "description": "Mac M1/M2/M3 with 8GB RAM (minimal) - 内存最小配置",
      "min_memory_gb": 8,
      "max_memory_gb": 12,
      "settings": {
        "batch_size": 1,
        "num_layers": 4,
        "iters": 200,
        "max_seq_length": 512,
        "learning_rate": 1e-5
      }
    },
    "mac_m1_16gb": {
      "description": "Mac M1/M2/M3 with 16GB RAM (standard) - 标准配置",
      "min_memory_gb": 12,
      "max_memory_gb": 24,
      "settings": {
        "batch_size": 1,
        "num_layers": 8,
        "iters": 400,
        "max_seq_length": 1024,
        "learning_rate": 1e-5
      }
    },
    "mac_m1_32gb": {
      "description": "Mac M1 Max/M2 Max/M3 Max with 32GB+ RAM - 高配置",
      "min_memory_gb": 24,
      "max_memory_gb": 64,
      "settings": {
        "batch_size": 2,
        "num_layers": 12,
        "iters": 600,
        "max_seq_length": 1536,
        "learning_rate": 1e-5
      }
    },
    "mac_m1_64gb": {
      "description": "Mac M1/M2/M3 Ultra with 64GB+ RAM - 超高配置",
      "min_memory_gb": 64,
      "max_memory_gb": 128,
      "settings": {
        "batch_size": 4,
        "num_layers": 16,
        "iters": 1000,
        "max_seq_length": 2048,
        "learning_rate": 1e-5
      }
    },
    "mac_m1_128gb": {
      "description": "Mac M1/M2/M3 Ultra with 128GB+ RAM - 专业配置",
      "min_memory_gb": 128,
      "max_memory_gb": 256,
      "settings": {
        "batch_size": 8,
        "num_layers": 24,
        "iters": 1500,
        "max_seq_length": 4096,
        "learning_rate": 1e-5
      }
    },
    "mac_memory_safe": {
      "description": "Memory-safe profile for any Mac - 内存安全配置（推荐用于OOM问题）",
      "min_memory_gb": 0,
      "max_memory_gb": 999,
      "settings": {
        "batch_size": 1,
        "num_layers": 4,
        "iters": 300,
        "max_seq_length": 768,
        "learning_rate": 1e-5
      }
    },
    "linux_gpu_8gb": {
      "description": "Linux with 8GB VRAM GPU (RTX 3070, etc.)",
      "min_vram_gb": 8,
      "max_vram_gb": 12,
      "settings": {
        "batch_size": 2,
        "gradient_accumulation_steps": 8,
        "lora_r": 8,
        "lora_alpha": 16,
        "max_length": 1024,
        "quantize": "4bit"
      }
    },
    "linux_gpu_16gb": {
      "description": "Linux with 16GB VRAM GPU (RTX 4080, etc.)",
      "min_vram_gb": 12,
      "max_vram_gb": 20,
      "settings": {
        "batch_size": 4,
        "gradient_accumulation_steps": 4,
        "lora_r": 16,
        "lora_alpha": 32,
        "max_length": 2048,
        "quantize": "4bit"
      }
    },
    "linux_gpu_24gb": {
      "description": "Linux with 24GB VRAM GPU (RTX 4090, A5000, etc.)",
      "min_vram_gb": 20,
      "max_vram_gb": 32,
      "settings": {
        "batch_size": 8,
        "gradient_accumulation_steps": 2,
        "lora_r": 16,
        "lora_alpha": 32,
        "max_length": 2048,
        "quantize": "8bit"
      }
    },
    "linux_gpu_48gb": {
      "description": "Linux with 48GB+ VRAM GPU (A6000, H100, etc.)",
      "min_vram_gb": 40,
      "max_vram_gb": 256,
      "settings": {
        "batch_size": 16,
        "gradient_accumulation_steps": 1,
        "lora_r": 32,
        "lora_alpha": 64,
        "max_length": 4096,
        "quantize": "none"
      }
    },
    "windows_gpu_8gb": {
      "description": "Windows with 8GB VRAM GPU (RTX 3070, etc.)",
      "platform": "windows",
      "min_vram_gb": 8,
      "max_vram_gb": 12,
      "settings": {
        "batch_size": 1,
        "gradient_accumulation_steps": 8,
        "lora_r": 8,
        "lora_alpha": 16,
        "max_length": 1024,
        "quantize": "4bit"
      }
    },
    "windows_gpu_16gb": {
      "description": "Windows with 16GB VRAM GPU (RTX 4080, etc.)",
      "platform": "windows",
      "min_vram_gb": 12,
      "max_vram_gb": 20,
      "settings": {
        "batch_size": 2,
        "gradient_accumulation_steps": 4,
        "lora_r": 16,
        "lora_alpha": 32,
        "max_length": 2048,
        "quantize": "4bit"
      }
    },
    "windows_gpu_24gb": {
      "description": "Windows with 24GB VRAM GPU (RTX 4090, etc.)",
      "platform": "windows",
      "min_vram_gb": 20,
      "max_vram_gb": 48,
      "settings": {
        "batch_size": 4,
        "gradient_accumulation_steps": 2,
        "lora_r": 16,
        "lora_alpha": 32,
        "max_length": 2048,
        "quantize": "4bit"
      }
    },
    "cpu_only": {
      "description": "CPU-only training (very slow, for testing)",
      "platform": "any",
      "settings": {
        "batch_size": 1,
        "gradient_accumulation_steps": 32,
        "lora_r": 4,
        "lora_alpha": 8,
        "max_length": 256,
        "quantize": "8bit",
        "epochs": 1
      }
    },
    "conservative": {
      "description": "Conservative settings for any device (slow but safe)",
      "settings": {
        "batch_size": 1,
        "num_layers": 4,
        "iters": 200,
        "max_seq_length": 512,
        "learning_rate": 1e-5,
        "gradient_accumulation_steps": 16,
        "lora_r": 8,
        "lora_alpha": 16,
        "max_length": 512,
        "quantize": "4bit"
      }
    }
  }
}
