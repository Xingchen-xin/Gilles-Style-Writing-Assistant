# GSWA Fine-tuning Guide / å¾®è°ƒæŒ‡å—

## TL;DR å‚»ç“œå¼æ“ä½œ (4 Steps)

```bash
# åªéœ€ 4 æ­¥ / Just 4 steps:

# 1. æ”¾æ–‡ç« åˆ°æ–‡ä»¶å¤¹ / Add your documents
#    data/corpus/raw/                    <- æ™®é€šæ–‡ç«  / Regular articles
#    data/corpus/raw/important_examples/ <- é‡è¦æ–‡ç«  (2.5xæƒé‡) / Important examples

# 2. ç”Ÿæˆé£æ ¼è½¬æ¢å¯¹ / Generate style-transfer pairs (ä¸€æ¬¡æ€§ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ )
make parse-corpus
make generate-pairs OLLAMA_MODEL=qwen3-coder:30b

# 3. ä¸€é”®æ™ºèƒ½è®­ç»ƒ / One-click smart training
make finetune-smart

# 4. æŒ‰ç…§è¾“å‡ºæç¤ºå®Œæˆé…ç½® / Follow the output instructions
```

### ğŸš€ æ™ºèƒ½è®­ç»ƒç‰¹æ€§ / Smart Training Features

- **è‡ªåŠ¨æ£€æµ‹å¹³å°**: Mac â†’ MLX, Linux/Windows â†’ LoRA
- **è‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶**: GPUå‹å·ã€æ˜¾å­˜å¤§å°ã€ç³»ç»Ÿå†…å­˜
- **è‡ªåŠ¨é€‰æ‹©å‚æ•°**: batch_size, learning_rate, é‡åŒ–ç­‰çº§
- **è‡ªåŠ¨æ¨èæ¨¡å‹**: æ ¹æ®ç¡¬ä»¶æ¨èæœ€ä½³åŸºåº•æ¨¡å‹

---

## æ–‡ä»¶å¤¹ç»“æ„è¯´æ˜

```
data/corpus/raw/                      <- æ™®é€š Gilles æ–‡ç« 
â”œâ”€â”€ paper1.pdf
â”œâ”€â”€ paper2.docx
â”œâ”€â”€ paper3.txt
â”‚
â””â”€â”€ important_examples/               <- é‡è¦/ä»£è¡¨æ€§æ–‡ç«  (è‡ªåŠ¨ 2.5x æƒé‡)
    â”œâ”€â”€ best_review.pdf
    â””â”€â”€ classic_paper.pdf
```

**æƒé‡è¯´æ˜ï¼š**
| ä½ç½® | è‡ªåŠ¨æƒé‡ | è¯´æ˜ |
|------|----------|------|
| `raw/` | 1.0x | æ™®é€šæ–‡ç«  |
| `raw/important_examples/` | 2.5x | é‡è¦æ–‡ç« ï¼Œè®­ç»ƒæ—¶å‡ºç°æ›´å¤šæ¬¡ |

**æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š** `.pdf`, `.docx`, `.txt`

---

## ä¸ºä»€ä¹ˆéœ€è¦å¾®è°ƒï¼Ÿ

å½“å‰é—®é¢˜ï¼š
1. **AI æ£€æµ‹å™¨è¯†åˆ«** - ç”Ÿæˆçš„æ–‡æœ¬è¢«è¯†åˆ«ä¸ºçº¯ AI ç”Ÿæˆ
2. **é£æ ¼ä¸åŒ¹é…** - è¾“å‡ºä¸åƒ Gilles çš„å†™ä½œé£æ ¼
3. **é€šç”¨æ€§è¿‡å¼º** - æ¨¡å‹æ²¡æœ‰å­¦ä¹  Gilles ç‰¹æœ‰çš„è¡¨è¾¾æ–¹å¼

è§£å†³æ–¹æ¡ˆï¼š**Style-Transfer Fine-tuning (é£æ ¼è½¬æ¢å¾®è°ƒ)**

### è®­ç»ƒåŸç†

ä½¿ç”¨ **Approach B: Synthetic Pairs** æ–¹æ³•ï¼š
1. ç”¨æœ¬åœ° LLM å°† Gilles çš„æ¯ä¸ªæ®µè½"ç®€åŒ–"ä¸ºé€šç”¨å­¦æœ¯è‹±è¯­
2. è®­ç»ƒæ¨¡å‹å­¦ä¹ ä»{é€šç”¨è¾“å…¥ â†’ Gilles é£æ ¼è¾“å‡º}çš„æ˜ å°„
3. ä½¿ç”¨æ¨¡å‹åŸç”Ÿ chat template (`[INST]...[/INST]`) ç¡®ä¿è®­ç»ƒå’Œæ¨ç†æ ¼å¼ä¸€è‡´
4. Label masking ç¡®ä¿åªè®­ç»ƒ response tokens

**ç¤ºä¾‹:**
```
Input (é€šç”¨):  "SEM analysis confirmed earlier aerial hyphae development in the mutant."
Output (Gilles): "The precocious erection of aerial hyphae in the redD mutant was confirmed
                  by scanning electron microscopy (SEM)."
```

æ¨¡å‹å­¦åˆ°çš„è½¬æ¢ï¼š
- "earlier development" â†’ "precocious erection" (ç²¾ç¡®ã€ç”ŸåŠ¨çš„è¯æ±‡)
- è¢«åŠ¨å¥ â†’ å¤æ‚ä»å±ç»“æ„
- æ·»åŠ  discourse markers (Indeed, Notably, Together)

---

## è·¨å¹³å°æ”¯æŒ / Multi-Platform Support

| å¹³å° | è®­ç»ƒæ–¹å¼ | æ£€æµ‹å‘½ä»¤ | è¯´æ˜ |
|------|----------|----------|------|
| **Mac** (M1/M2/M3/M4) | MLX | `make check-mlx` | Apple Silicon ä¸“ç”¨ä¼˜åŒ– |
| **Linux** (NVIDIA GPU) | LoRA/QLoRA | `make check-lora` | CUDA åŠ é€Ÿè®­ç»ƒ |
| **Windows** (NVIDIA GPU) | LoRA/QLoRA | `make check-lora` | éœ€å®‰è£… CUDA |
| **æ—  GPU** | CPU LoRA | - | éå¸¸æ…¢ï¼Œä»…ä¾›æµ‹è¯• |

## æ¨èåŸºåº•æ¨¡å‹ / Recommended Base Models

| æ˜¾å­˜/å†…å­˜ | æ¨èæ¨¡å‹ | è¯´æ˜ |
|-----------|----------|------|
| 8GB | `Qwen/Qwen2.5-1.5B-Instruct` | æœ€å°å¯ç”¨ï¼ŒåŸºç¡€è´¨é‡ |
| 16GB | `mistralai/Mistral-7B-Instruct-v0.3` | æ¨èå…¥é—¨ç”¨æˆ· |
| 24GB+ | `mistralai/Mistral-Nemo-Instruct-2407` | **æ¨è** - 12B æ¨¡å‹ï¼Œæœ€ä½³æ€§ä»·æ¯” |
| 48GB+ | `mistralai/Mistral-Large-Instruct-2407` | é«˜è´¨é‡è¾“å‡º |
| 60GB+ | `meta-llama/Llama-3.3-70B-Instruct` | å¯é€‰ (éœ€è¦ `--model llama3.3`) |

**ä¸ºä»€ä¹ˆæ¨è Mistral-Nemo 12B?**
- **æ¨¡å‹å®¹é‡ä¸æ•°æ®é‡åŒ¹é…**: 12B å‚æ•°å¯¹ ~1000 æ ·æœ¬æ›´åˆé€‚ï¼Œé¿å…è¿‡æ‹Ÿåˆ
- **å¯ç”¨æ›´å¤§ batch size**: batch=4 vs 70B çš„ batch=1ï¼Œæ¢¯åº¦æ›´ç¨³å®š
- **è®­ç»ƒé€Ÿåº¦å¿«**: æ¯” 70B å¿« 3-4 å€
- **è‹±æ–‡å­¦æœ¯å†™ä½œè´¨é‡ä¼˜ç§€**: åœ¨ç§‘å­¦å†™ä½œä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²
- **æ”¯æŒé•¿ä¸Šä¸‹æ–‡**: 32K tokens

**å…³äº 70B+ å¤§æ¨¡å‹**
- 70B æ¨¡å‹é€‚åˆæ•°æ®é‡å……è¶³ (>5000 æ ·æœ¬) çš„åœºæ™¯
- å¯¹äº ~1000 æ ·æœ¬çš„æ•°æ®é›†ï¼Œ12B æ¨¡å‹é€šå¸¸æ•ˆæœæ›´å¥½
- å¦‚éœ€ä½¿ç”¨ 70Bï¼Œè¯·æ˜¾å¼æŒ‡å®š: `--model llama3.3`
- å¤š GPU ç³»ç»Ÿä¼šè‡ªåŠ¨å¯ç”¨ DeepSpeed ZeRO-3

```bash
# è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆæ¨è Mistral-Nemo 12Bï¼‰
make finetune-smart

# åå°è¿è¡Œï¼ˆä¿å­˜æ—¥å¿—ï¼‰
python scripts/smart_finetune.py --background -y

# æ‰‹åŠ¨æŒ‡å®š 70B æ¨¡å‹ï¼ˆéœ€è¦å¤§é‡æ•°æ®ï¼‰
python scripts/smart_finetune.py --model llama3.3 -y

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f logs/finetune-background-*.log
```

**å…³äº Mistral tokenizer è­¦å‘Š**
- Mistral ç³»åˆ—æ¨¡å‹çš„ tokenizer è­¦å‘Šå·²è‡ªåŠ¨å¤„ç†
- è„šæœ¬ä¼šè‡ªåŠ¨åº”ç”¨ `fix_mistral_regex=True` å¹¶æŠ‘åˆ¶è­¦å‘Š

## è®­ç»ƒå‚æ•°è¯´æ˜

| å‚æ•° | Mistral-Nemo 12B | Llama 70B | è¯´æ˜ |
|------|------------------|-----------|------|
| batch_size | 4 | 1 | æ›´å¤§ batch = æ›´ç¨³å®šæ¢¯åº¦ |
| gradient_accumulation | 4 | 8 | æœ‰æ•ˆ batch = batch Ã— accum |
| lora_r | 32 | 16 | LoRA ç§©ï¼Œè¶Šå¤§å®¹é‡è¶Šå¤§ |
| lora_alpha | 64 | 32 | é€šå¸¸ = 2 Ã— lora_r |
| learning_rate | 1e-4 | 5e-5 | QLoRA æ ‡å‡†å€¼ |
| epochs | 3 | 1-2 | æ ¹æ®æ•°æ®é‡è°ƒæ•´ |
| max_length | 2048 | 1024 | å­¦æœ¯æ–‡ç« é€šå¸¸è¾ƒé•¿ |

## å¾®è°ƒæ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | ç¡¬ä»¶è¦æ±‚ | è®­ç»ƒæ—¶é—´ | è´¨é‡ | éš¾åº¦ | æ¨èåœºæ™¯ |
|------|----------|----------|------|------|----------|
| **MLX (Mac)** | M1/M2/M3 16GB+ | 1-2å°æ—¶ | â­â­â­â­ | ä½ | **Mac ç”¨æˆ·é¦–é€‰** |
| **LoRA** | GPU 16GB+ | 2-4å°æ—¶ | â­â­â­â­ | ä¸­ | Linux/Windows |
| **QLoRA** | GPU 8GB+ | 3-6å°æ—¶ | â­â­â­ | ä¸­ | æ˜¾å­˜æœ‰é™ |
| **Full Fine-tuning** | GPU 48GB+ | 8-24å°æ—¶ | â­â­â­â­â­ | é«˜ | æœ€ä½³è´¨é‡ |

---

## Mac ç”¨æˆ·å‚»ç“œå¼æ•™ç¨‹

### ç¬¬ä¸€æ­¥ï¼šæ”¾å…¥æ–‡ç« 

1. æ‰“å¼€ Finderï¼Œè¿›å…¥é¡¹ç›®ç›®å½•
2. æ‰“å¼€ `data/corpus/raw/` æ–‡ä»¶å¤¹
3. æŠŠ Gilles çš„ PDF æ–‡ç« æ‹–è¿›å»
4. å¦‚æœæœ‰æœ€èƒ½ä»£è¡¨ Gilles é£æ ¼çš„æ–‡ç« ï¼Œæ”¾å…¥ `raw/important_examples/`

```bash
# æˆ–è€…ç”¨å‘½ä»¤è¡Œ
cp ~/Downloads/*.pdf data/corpus/raw/

# é‡è¦æ–‡ç« æ”¾è¿™é‡Œ
cp ~/Downloads/important_paper.pdf data/corpus/raw/important_examples/
```

### ç¬¬äºŒæ­¥ï¼šå®‰è£…ä¾èµ–ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰

```bash
# å®‰è£… MLXï¼ˆApple Silicon ä¸“ç”¨æœºå™¨å­¦ä¹ åº“ï¼‰
pip install mlx mlx-lm
```

### ç¬¬ä¸‰æ­¥ï¼šä¸€é”®å¾®è°ƒ

```bash
# è¿™ä¸€æ¡å‘½ä»¤å®Œæˆæ‰€æœ‰å·¥ä½œï¼šè§£ææ–‡ç«  â†’ ç”Ÿæˆè®­ç»ƒæ•°æ® â†’ å¾®è°ƒæ¨¡å‹
make finetune-all
```

**çœ‹åˆ°çš„è¾“å‡ºï¼š**
```
============================================================
GSWA Corpus Parser
============================================================
Input (regular):  ./data/corpus/raw
Input (priority): ./data/corpus/raw/important_examples

Found 15 documents:
  - Regular articles:  12
  - Priority articles: 3 (in important_examples/)

Processing: paper1.pdf...
  Extracted 45 paragraphs
Processing: best_review.pdf â­...
  Extracted 120 paragraphs
...

============================================================
Starting MLX Fine-tuning
============================================================
Epoch 1/3: loss=2.45
Epoch 2/3: loss=1.89
Epoch 3/3: loss=1.23

Model saved to: models/gswa-mlx-mistral/
```

### ç¬¬å››æ­¥ï¼šåˆ›å»º Ollama æ¨¡å‹

```bash
# æ ¹æ®è¾“å‡ºçš„æ¨¡å‹è·¯å¾„åˆ›å»º Ollama æ¨¡å‹
ollama create gswa-gilles -f models/gswa-mlx-mistral/Modelfile
```

### ç¬¬äº”æ­¥ï¼šæ›´æ–°é…ç½®å¹¶è¿è¡Œ

```bash
# æ›´æ–° .env ä½¿ç”¨æ–°æ¨¡å‹
echo "VLLM_MODEL_NAME=gswa-gilles" >> .env

# é‡å¯ GSWA
make run
```

**æ­å–œï¼ç°åœ¨ GSWA ä½¿ç”¨çš„æ˜¯å¾®è°ƒåçš„æ¨¡å‹ï¼**

---

## Linux ç”¨æˆ·å‚»ç“œå¼æ•™ç¨‹

### ç¬¬ä¸€æ­¥ï¼šæ”¾å…¥æ–‡ç« 

åŒ Mac ç”¨æˆ·ï¼Œæ”¾å…¥ `data/corpus/raw/` å’Œ `raw/important_examples/`

### ç¬¬äºŒæ­¥ï¼šå®‰è£…ä¾èµ–

```bash
# å®‰è£…è®­ç»ƒä¾èµ–
make setup-cuda-auto
# æˆ–æ‰‹åŠ¨
micromamba create -n gswa python=3.11 -y && micromamba activate gswa
pip install -e ".[dev,similarity]" pymupdf
```

### ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆ Style-Transfer Pairs

```bash
# è§£æè¯­æ–™åº“
make parse-corpus

# ç”Ÿæˆé£æ ¼å¯¹ (ä¸€æ¬¡æ€§æ“ä½œï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œ~4å°æ—¶)
make generate-pairs OLLAMA_MODEL=qwen3-coder:30b

# æˆ–åå°è¿è¡Œ
nohup micromamba run -n gswa python -u scripts/prepare_training_data.py \
    --generate-pairs --ollama-model qwen3-coder:30b > /tmp/pair_generation.log 2>&1 &
tail -f /tmp/pair_generation.log  # ç›‘æ§è¿›åº¦
```

### ç¬¬å››æ­¥ï¼šè®­ç»ƒæ¨¡å‹

```bash
# ä¸€é”®æ™ºèƒ½è®­ç»ƒï¼ˆè‡ªåŠ¨æ£€æµ‹GPUå¹¶é€‰æ‹©å‚æ•°ï¼‰
make finetune-smart

# åå°è®­ç»ƒï¼ˆæ¨èï¼Œå…³é—­ç»ˆç«¯ä¸ä¸­æ–­ï¼‰
make finetune-background
```

### ç¬¬äº”æ­¥ï¼šè¯„ä¼°å’Œéƒ¨ç½²

```bash
# è¯„ä¼°æ¨¡å‹æ•ˆæœ
make evaluate MODEL_DIR=models/gswa-lora-Mistral-<timestamp>

# æŸ¥çœ‹è®­ç»ƒæ›²çº¿
make visualize MODEL_DIR=models/gswa-lora-Mistral-<timestamp>

# éƒ¨ç½²ï¼šé…ç½® .env ä½¿ç”¨ LoRA adapter
LORA_ADAPTER_PATH=./models/gswa-lora-Mistral-<timestamp>
```

---

## Windows ç”¨æˆ·å‚»ç“œå¼æ•™ç¨‹

### å‰ç½®è¦æ±‚

1. **NVIDIA GPU** (8GB+ VRAM)
2. **CUDA Toolkit** (æ¨è 12.1+)
3. **Python 3.10+**

### ç¬¬ä¸€æ­¥ï¼šå®‰è£… CUDA

1. ä¸‹è½½ [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)
2. å®‰è£…å¹¶é‡å¯
3. éªŒè¯: `nvidia-smi`

### ç¬¬äºŒæ­¥ï¼šå®‰è£… PyTorch with CUDA

```powershell
pip install torch --index-url https://download.pytorch.org/whl/cu121
pip install transformers peft datasets accelerate bitsandbytes-windows
```

### ç¬¬ä¸‰æ­¥ï¼šæ”¾å…¥æ–‡ç« 

åŒ Mac/Linux ç”¨æˆ·ï¼Œæ”¾å…¥ `data/corpus/raw/` å’Œ `raw/important_examples/`

### ç¬¬å››æ­¥ï¼šä¸€é”®æ™ºèƒ½å¾®è°ƒ

```powershell
# åœ¨ PowerShell æˆ– CMD ä¸­è¿è¡Œ
python scripts/smart_finetune.py
```

æˆ–è€…ä½¿ç”¨ make (éœ€å®‰è£… [GNU Make for Windows](http://gnuwin32.sourceforge.net/packages/make.htm)):
```powershell
make finetune-smart
```

---

## ç¡¬ä»¶é…ç½®æ–‡ä»¶ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰

ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹ä½ çš„ç¡¬ä»¶å¹¶é€‰æ‹©æœ€ä½³è®­ç»ƒå‚æ•°ï¼š

| ç¡¬ä»¶ | å†…å­˜ | batch_size | num_layers | iters |
|------|------|------------|------------|-------|
| M1/M2/M3 8GB | 8GB | 1 | 4 | 300 |
| M1/M2/M3 16GB | 16GB | 2 | 8 | 500 |
| M1/M2/M3 Max 32GB+ | 32GB+ | 4 | 16 | 1000 |
| M1/M2/M3 Ultra 64GB+ | 64GB+ | 8 | 32 | 1500 |

**è‡ªå®šä¹‰é…ç½®ï¼š** ç¼–è¾‘ `config/training_profiles.json` æ–‡ä»¶ã€‚

**æŸ¥çœ‹ç³»ç»Ÿæ£€æµ‹ç»“æœï¼š**
```bash
python scripts/finetune_mlx_mac.py --auto --check-only
```

---

## æ‰‹åŠ¨é…ç½®æƒé‡ï¼ˆé«˜çº§ï¼‰

å¦‚æœä½ æƒ³ç²¾ç¡®æ§åˆ¶æ¯ç¯‡æ–‡ç« çš„æƒé‡ï¼Œå¯ä»¥ç¼–è¾‘ `data/corpus/priority_weights.json`ï¼š

```json
{
  "default_weight": 1.0,
  "priority_folder_weight": 2.5,

  "priority_docs": {
    "Barka_MicrobiolMolBiolRev2016": {
      "weight": 3.0,
      "reason": "æœ€èƒ½ä»£è¡¨ Gilles é£æ ¼çš„ç»¼è¿°"
    }
  },

  "exclude_docs": {
    "some_bad_paper": {
      "reason": "å¤ªçŸ­ï¼Œä¸èƒ½ä»£è¡¨é£æ ¼"
    }
  }
}
```

**æŸ¥çœ‹æ‰€æœ‰æ–‡ç«  IDï¼š**
```bash
make list-docs
```

---

## å®Œæ•´ Makefile å‘½ä»¤

```bash
# === è¯­æ–™ç®¡ç† ===
make corpus            # æŸ¥çœ‹è¯­æ–™åº“çŠ¶æ€
make corpus-guide      # æ˜¾ç¤ºæ·»åŠ æ–‡ä»¶æŒ‡å—
make corpus-validate   # éªŒè¯æ‰€æœ‰è¯­æ–™æ–‡ä»¶
make parse-corpus      # è§£æ raw/ ä¸­çš„æ–‡ç« 
make list-docs         # åˆ—å‡ºæ‰€æœ‰æ–‡ç«  ID
make training-stats    # æŸ¥çœ‹è®­ç»ƒæ•°æ®ç»Ÿè®¡

# === æ•°æ®å‡†å¤‡ ===
make generate-pairs    # ç”Ÿæˆ style-transfer pairs (ä¸€æ¬¡æ€§ï¼Œ~4å°æ—¶)
make prepare-training  # ä» pairs ç”Ÿæˆ Alpaca æ ¼å¼è®­ç»ƒæ•°æ®

# === æ™ºèƒ½è®­ç»ƒ ===
make finetune-smart    # ä¸€é”®æ™ºèƒ½è®­ç»ƒï¼ˆè‡ªåŠ¨æ£€æµ‹å¹³å°å’Œç¡¬ä»¶ï¼‰
make finetune-background  # åå°è®­ç»ƒï¼ˆå…³é—­ç»ˆç«¯ä¸ä¸­æ–­ï¼‰
make finetune-all      # Mac ä¸€é”®è®­ç»ƒï¼ˆparse + prepare + mlxï¼‰

# === è¯„ä¼°å’Œå¯è§†åŒ– ===
make visualize MODEL_DIR=models/gswa-lora-...  # è®­ç»ƒæ›²çº¿
make evaluate MODEL_DIR=models/gswa-lora-...   # ç”Ÿæˆæ ·æœ¬è¯„ä¼°
make compare-runs      # å¤šæ¬¡è®­ç»ƒå¯¹æ¯”

# === åˆ†æ­¥è®­ç»ƒ ===
make finetune-mlx      # Mac MLX å¾®è°ƒ
make finetune-lora     # Linux/Windows LoRA å¾®è°ƒ
make finetune-deepspeed  # å¤šå¡ 70B+ æ¨¡å‹

# === ç¯å¢ƒæ£€æŸ¥ ===
make check-mlx         # æ£€æŸ¥ MLX ä¾èµ– (Mac)
make check-lora        # æ£€æŸ¥ LoRA ä¾èµ– (Linux/Windows)
make train-info        # æŸ¥çœ‹ç¡¬ä»¶ä¿¡æ¯å’Œæ¨è
```

---

## å¦‚ä½•å‡å°‘ AI æ£€æµ‹ï¼Ÿ

å¾®è°ƒåçš„æ¨¡å‹ä¼šæ›´å¥½åœ°æ¨¡ä»¿äººç±»å†™ä½œé£æ ¼ï¼Œä½†è¿˜å¯ä»¥é‡‡å–ä»¥ä¸‹æªæ–½ï¼š

### 1. ä½¿ç”¨é«˜è´¨é‡è¯­æ–™

- æ”¾å…¥æ›´å¤š Gilles çš„æ–‡ç« ï¼ˆè¶Šå¤šè¶Šå¥½ï¼‰
- æŠŠæœ€èƒ½ä»£è¡¨é£æ ¼çš„æ”¾å…¥ `important_examples/`
- æ’é™¤ä¸å…¸å‹çš„æ–‡ç« 

### 2. è°ƒæ•´ç”Ÿæˆå‚æ•°

åœ¨ `.env` ä¸­è®¾ç½®ï¼š
```bash
TEMPERATURE_BASE=0.4      # ç•¥é«˜çš„æ¸©åº¦å¢åŠ å˜åŒ–
TEMPERATURE_VARIANCE=0.2  # å˜ä½“é—´æ›´å¤§å·®å¼‚
```

### 3. åå¤„ç†

- è½»å¾®ç¼–è¾‘ç”Ÿæˆçš„æ–‡æœ¬
- æ·»åŠ ä¸ªäººè¡¨è¾¾
- è°ƒæ•´å¥å­ç»“æ„

---

## DPO è¿›é˜¶è®­ç»ƒï¼ˆåå¥½å¯¹é½ï¼‰

ä½¿ç”¨åï¼Œä½ å¯ä»¥é€šè¿‡åé¦ˆè¿›ä¸€æ­¥ä¼˜åŒ–ï¼š

1. ä½¿ç”¨ GSWA ç”Ÿæˆå˜ä½“
2. åœ¨ UI ä¸­ä¸ºå˜ä½“è¯„åˆ†ï¼ˆBest/Good/Badï¼‰
3. æäº¤åé¦ˆ
4. å¯¼å‡ºå¹¶è®­ç»ƒï¼š

```bash
make export-dpo
python scripts/prepare_training_data.py --format dpo --from-feedback
make finetune-lora
```

---

## æ•…éšœæ’é™¤

### Q: æ²¡æœ‰æ£€æµ‹åˆ°æ–‡ç« ï¼Ÿ

A: æ£€æŸ¥æ–‡ä»¶ä½ç½®å’Œæ ¼å¼ï¼š
```bash
ls data/corpus/raw/
ls data/corpus/raw/important_examples/
```
ç¡®ä¿æ˜¯ `.pdf`, `.docx`, æˆ– `.txt` æ–‡ä»¶ã€‚

### Q: MLX è®­ç»ƒå¤ªæ…¢ï¼Ÿ

A: å‡å°‘è¿­ä»£æ¬¡æ•°æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼š
```bash
python scripts/finetune_mlx_mac.py --model phi --iters 500
```

### Q: å†…å­˜ä¸è¶³ (Mac)?

A: ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹ä½ çš„ç¡¬ä»¶å¹¶é€‰æ‹©åˆé€‚çš„é…ç½®ã€‚å¦‚éœ€æ‰‹åŠ¨è°ƒæ•´ï¼š
```bash
# æŸ¥çœ‹å¯ç”¨é…ç½®
python scripts/finetune_mlx_mac.py --list-profiles

# ä½¿ç”¨ä¿å®ˆé…ç½®ï¼ˆæœ€ä½å†…å­˜ï¼‰
python scripts/finetune_mlx_mac.py --profile conservative

# æˆ–è€…æ‰‹åŠ¨è®¾ç½®å‚æ•°
python scripts/finetune_mlx_mac.py --batch-size 1 --num-layers 4 --max-seq-length 512
```

### Q: æ˜¾å­˜ä¸è¶³ (CUDA OOM)ï¼Ÿ

A: ä½¿ç”¨ 4-bit é‡åŒ–ï¼š
```bash
python scripts/finetune_lora.py --quantize 4bit --batch-size 1
```

### Q: è®­ç»ƒå¡åœ¨ 0%ï¼Ÿ/ Training stuck at 0%?

A: å¯èƒ½çš„åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š

1. **Mistral æ¨¡å‹å…¼å®¹æ€§é—®é¢˜** - å·²åœ¨æœ€æ–°ç‰ˆæœ¬ä¸­ä¿®å¤
   ```bash
   git pull  # æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬
   ```

2. **æ˜¾å­˜ä¸è¶³** - å°è¯•ä½¿ç”¨æ›´å°çš„æ¨¡å‹
   ```bash
   python scripts/smart_finetune.py --model mistral  # ä½¿ç”¨ 7B æ¨¡å‹
   ```

3. **å¤š GPU å†²çª** - å¼ºåˆ¶ä½¿ç”¨å•å¡
   ```bash
   CUDA_VISIBLE_DEVICES=0 python scripts/finetune_lora.py --model mistral
   ```

4. **æ¢¯åº¦æ£€æŸ¥ç‚¹é—®é¢˜** - è„šæœ¬å·²è‡ªåŠ¨å¤„ç† Mistral æ¨¡å‹çš„å…¼å®¹æ€§

5. **æ—¥å¿—ä¸­åªæ˜¾ç¤º 0%** - tqdm åœ¨æ—¥å¿—æ–‡ä»¶ä¸­ä¸ä¼šæŒç»­åˆ·æ–°
   ```bash
   # å…³é—­ tqdm å¹¶å¼ºåˆ¶æ¯æ­¥è¾“å‡º
   python scripts/finetune_lora.py --disable-tqdm --log-every 1
   ```

### Q: ç”Ÿæˆè´¨é‡ä¸‹é™ï¼Ÿ

A: å¯èƒ½æ˜¯è¿‡æ‹Ÿåˆï¼Œå°è¯•ï¼š
- å‡å°‘è®­ç»ƒè½®æ•°
- å¢åŠ æ›´å¤šæ–‡ç« 
- ä½¿ç”¨éªŒè¯é›†

### Q: å¦‚ä½•å›æ»šåˆ°åŸæ¨¡å‹ï¼Ÿ

A: ä¿®æ”¹ `.env`ï¼š
```bash
VLLM_MODEL_NAME=mistral  # ä½¿ç”¨åŸå§‹æ¨¡å‹
```

---

## å‚è€ƒèµ„æº

- [LoRA è®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [QLoRA è®ºæ–‡](https://arxiv.org/abs/2305.14314)
- [DPO è®ºæ–‡](https://arxiv.org/abs/2305.18290)
- [MLX æ–‡æ¡£](https://ml-explore.github.io/mlx/)
- [Ollama æ¨¡å‹åˆ›å»º](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)
