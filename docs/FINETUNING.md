# GSWA Fine-tuning Guide / å¾®è°ƒæŒ‡å—

## TL;DR å‚»ç“œå¼æ“ä½œ (3 Steps)

```bash
# åªéœ€ 3 æ­¥ / Just 3 steps:

# 1. æ”¾æ–‡ç« åˆ°æ–‡ä»¶å¤¹ / Add your documents
#    data/corpus/raw/                    <- æ™®é€šæ–‡ç«  / Regular articles
#    data/corpus/raw/important_examples/ <- é‡è¦æ–‡ç«  (2.5xæƒé‡) / Important examples

# 2. ä¸€é”®æ™ºèƒ½è®­ç»ƒ / One-click smart training (works on Mac/Linux/Windows!)
make finetune-smart

# 3. æŒ‰ç…§è¾“å‡ºæç¤ºå®Œæˆé…ç½® / Follow the output instructions
```

### ğŸš€ æ™ºèƒ½è®­ç»ƒç‰¹æ€§ / Smart Training Features

- **è‡ªåŠ¨æ£€æµ‹å¹³å°**: Mac â†’ MLX, Linux/Windows â†’ LoRA
- **è‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶**: GPUå‹å·ã€æ˜¾å­˜å¤§å°ã€ç³»ç»Ÿå†…å­˜
- **è‡ªåŠ¨é€‰æ‹©å‚æ•°**: batch_size, learning_rate, é‡åŒ–ç­‰çº§
- **è‡ªåŠ¨æ¨èæ¨¡å‹**: æ ¹æ®ç¡¬ä»¶æ¨èæœ€ä½³åŸºåº•æ¨¡å‹

---

## æ–‡ä»¶å¤¹ç»“æ„è¯´æ˜

```
data/corpus/raw/                      <- æ™®é€š Gilles æ–‡ç« 
â”œâ”€â”€ paper1.pdf
â”œâ”€â”€ paper2.docx
â”œâ”€â”€ paper3.txt
â”‚
â””â”€â”€ important_examples/               <- é‡è¦/ä»£è¡¨æ€§æ–‡ç«  (è‡ªåŠ¨ 2.5x æƒé‡)
    â”œâ”€â”€ best_review.pdf
    â””â”€â”€ classic_paper.pdf
```

**æƒé‡è¯´æ˜ï¼š**
| ä½ç½® | è‡ªåŠ¨æƒé‡ | è¯´æ˜ |
|------|----------|------|
| `raw/` | 1.0x | æ™®é€šæ–‡ç«  |
| `raw/important_examples/` | 2.5x | é‡è¦æ–‡ç« ï¼Œè®­ç»ƒæ—¶å‡ºç°æ›´å¤šæ¬¡ |

**æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š** `.pdf`, `.docx`, `.txt`

---

## ä¸ºä»€ä¹ˆéœ€è¦å¾®è°ƒï¼Ÿ

å½“å‰é—®é¢˜ï¼š
1. **AI æ£€æµ‹å™¨è¯†åˆ«** - ç”Ÿæˆçš„æ–‡æœ¬è¢«è¯†åˆ«ä¸ºçº¯ AI ç”Ÿæˆ
2. **é£æ ¼ä¸åŒ¹é…** - è¾“å‡ºä¸åƒ Gilles çš„å†™ä½œé£æ ¼
3. **é€šç”¨æ€§è¿‡å¼º** - æ¨¡å‹æ²¡æœ‰å­¦ä¹  Gilles ç‰¹æœ‰çš„è¡¨è¾¾æ–¹å¼

è§£å†³æ–¹æ¡ˆï¼š**å¾®è°ƒæ¨¡å‹ä½¿å…¶å­¦ä¹  Gilles çš„å†™ä½œé£æ ¼**

---

## è·¨å¹³å°æ”¯æŒ / Multi-Platform Support

| å¹³å° | è®­ç»ƒæ–¹å¼ | æ£€æµ‹å‘½ä»¤ | è¯´æ˜ |
|------|----------|----------|------|
| **Mac** (M1/M2/M3/M4) | MLX | `make check-mlx` | Apple Silicon ä¸“ç”¨ä¼˜åŒ– |
| **Linux** (NVIDIA GPU) | LoRA/QLoRA | `make check-lora` | CUDA åŠ é€Ÿè®­ç»ƒ |
| **Windows** (NVIDIA GPU) | LoRA/QLoRA | `make check-lora` | éœ€å®‰è£… CUDA |
| **æ—  GPU** | CPU LoRA | - | éå¸¸æ…¢ï¼Œä»…ä¾›æµ‹è¯• |

## æ¨èåŸºåº•æ¨¡å‹ / Recommended Base Models

| æ˜¾å­˜/å†…å­˜ | æ¨èæ¨¡å‹ | è¯´æ˜ |
|-----------|----------|------|
| 8GB | `Qwen/Qwen2.5-1.5B-Instruct` | æœ€å°å¯ç”¨ï¼ŒåŸºç¡€è´¨é‡ |
| 16GB | `Qwen/Qwen2.5-7B-Instruct` | **æ¨èå¤§å¤šæ•°ç”¨æˆ·** |
| 24GB | `Qwen/Qwen2.5-14B-Instruct` | æ›´å¥½çš„å†™ä½œè´¨é‡ |
| 48GB+ | `mistralai/Mistral-Large-Instruct-2407` | æœ€ä½³è´¨é‡ |

**ä¸ºä»€ä¹ˆæ¨è Qwen2.5?**
- åœ¨å­¦æœ¯å†™ä½œä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€
- å¯¹ä¸­è‹±æ–‡åŒè¯­æ”¯æŒè‰¯å¥½
- è®­ç»ƒæ•ˆç‡é«˜ï¼Œæ”¶æ•›å¿«

## å¾®è°ƒæ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | ç¡¬ä»¶è¦æ±‚ | è®­ç»ƒæ—¶é—´ | è´¨é‡ | éš¾åº¦ | æ¨èåœºæ™¯ |
|------|----------|----------|------|------|----------|
| **MLX (Mac)** | M1/M2/M3 16GB+ | 1-2å°æ—¶ | â­â­â­â­ | ä½ | **Mac ç”¨æˆ·é¦–é€‰** |
| **LoRA** | GPU 16GB+ | 2-4å°æ—¶ | â­â­â­â­ | ä¸­ | Linux/Windows |
| **QLoRA** | GPU 8GB+ | 3-6å°æ—¶ | â­â­â­ | ä¸­ | æ˜¾å­˜æœ‰é™ |
| **Full Fine-tuning** | GPU 48GB+ | 8-24å°æ—¶ | â­â­â­â­â­ | é«˜ | æœ€ä½³è´¨é‡ |

---

## Mac ç”¨æˆ·å‚»ç“œå¼æ•™ç¨‹

### ç¬¬ä¸€æ­¥ï¼šæ”¾å…¥æ–‡ç« 

1. æ‰“å¼€ Finderï¼Œè¿›å…¥é¡¹ç›®ç›®å½•
2. æ‰“å¼€ `data/corpus/raw/` æ–‡ä»¶å¤¹
3. æŠŠ Gilles çš„ PDF æ–‡ç« æ‹–è¿›å»
4. å¦‚æœæœ‰æœ€èƒ½ä»£è¡¨ Gilles é£æ ¼çš„æ–‡ç« ï¼Œæ”¾å…¥ `raw/important_examples/`

```bash
# æˆ–è€…ç”¨å‘½ä»¤è¡Œ
cp ~/Downloads/*.pdf data/corpus/raw/

# é‡è¦æ–‡ç« æ”¾è¿™é‡Œ
cp ~/Downloads/important_paper.pdf data/corpus/raw/important_examples/
```

### ç¬¬äºŒæ­¥ï¼šå®‰è£…ä¾èµ–ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰

```bash
# å®‰è£… MLXï¼ˆApple Silicon ä¸“ç”¨æœºå™¨å­¦ä¹ åº“ï¼‰
pip install mlx mlx-lm
```

### ç¬¬ä¸‰æ­¥ï¼šä¸€é”®å¾®è°ƒ

```bash
# è¿™ä¸€æ¡å‘½ä»¤å®Œæˆæ‰€æœ‰å·¥ä½œï¼šè§£ææ–‡ç«  â†’ ç”Ÿæˆè®­ç»ƒæ•°æ® â†’ å¾®è°ƒæ¨¡å‹
make finetune-all
```

**çœ‹åˆ°çš„è¾“å‡ºï¼š**
```
============================================================
GSWA Corpus Parser
============================================================
Input (regular):  ./data/corpus/raw
Input (priority): ./data/corpus/raw/important_examples

Found 15 documents:
  - Regular articles:  12
  - Priority articles: 3 (in important_examples/)

Processing: paper1.pdf...
  Extracted 45 paragraphs
Processing: best_review.pdf â­...
  Extracted 120 paragraphs
...

============================================================
Starting MLX Fine-tuning
============================================================
Epoch 1/3: loss=2.45
Epoch 2/3: loss=1.89
Epoch 3/3: loss=1.23

Model saved to: models/gswa-mlx-mistral/
```

### ç¬¬å››æ­¥ï¼šåˆ›å»º Ollama æ¨¡å‹

```bash
# æ ¹æ®è¾“å‡ºçš„æ¨¡å‹è·¯å¾„åˆ›å»º Ollama æ¨¡å‹
ollama create gswa-gilles -f models/gswa-mlx-mistral/Modelfile
```

### ç¬¬äº”æ­¥ï¼šæ›´æ–°é…ç½®å¹¶è¿è¡Œ

```bash
# æ›´æ–° .env ä½¿ç”¨æ–°æ¨¡å‹
echo "VLLM_MODEL_NAME=gswa-gilles" >> .env

# é‡å¯ GSWA
make run
```

**æ­å–œï¼ç°åœ¨ GSWA ä½¿ç”¨çš„æ˜¯å¾®è°ƒåçš„æ¨¡å‹ï¼**

---

## Linux ç”¨æˆ·å‚»ç“œå¼æ•™ç¨‹

### ç¬¬ä¸€æ­¥ï¼šæ”¾å…¥æ–‡ç« 

åŒ Mac ç”¨æˆ·ï¼Œæ”¾å…¥ `data/corpus/raw/` å’Œ `raw/important_examples/`

### ç¬¬äºŒæ­¥ï¼šå®‰è£…ä¾èµ–

```bash
# å®‰è£…è®­ç»ƒä¾èµ–
make install-train
```

### ç¬¬ä¸‰æ­¥ï¼šä¸€é”®æ™ºèƒ½å¾®è°ƒ

```bash
# ğŸš€ æ¨èï¼šæ™ºèƒ½è®­ç»ƒï¼ˆè‡ªåŠ¨æ£€æµ‹GPUå¹¶é€‰æ‹©å‚æ•°ï¼‰
make finetune-smart

# æˆ–è€…æ‰‹åŠ¨è¿è¡Œ LoRA è®­ç»ƒ
make finetune-lora
```

### ç¬¬å››æ­¥ï¼šéƒ¨ç½²æ¨¡å‹

å¾®è°ƒå®Œæˆåï¼Œæ¨¡å‹ä¿å­˜åœ¨ `models/gswa-lora-*/`ã€‚

```bash
# ä½¿ç”¨ PEFT åˆå¹¶æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
python scripts/merge_lora.py

# æˆ–è€…ç›´æ¥é…ç½® .env ä½¿ç”¨ LoRA adapter
LORA_ADAPTER_PATH=./models/gswa-lora-xxx
```

---

## Windows ç”¨æˆ·å‚»ç“œå¼æ•™ç¨‹

### å‰ç½®è¦æ±‚

1. **NVIDIA GPU** (8GB+ VRAM)
2. **CUDA Toolkit** (æ¨è 12.1+)
3. **Python 3.10+**

### ç¬¬ä¸€æ­¥ï¼šå®‰è£… CUDA

1. ä¸‹è½½ [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)
2. å®‰è£…å¹¶é‡å¯
3. éªŒè¯: `nvidia-smi`

### ç¬¬äºŒæ­¥ï¼šå®‰è£… PyTorch with CUDA

```powershell
pip install torch --index-url https://download.pytorch.org/whl/cu121
pip install transformers peft datasets accelerate bitsandbytes-windows
```

### ç¬¬ä¸‰æ­¥ï¼šæ”¾å…¥æ–‡ç« 

åŒ Mac/Linux ç”¨æˆ·ï¼Œæ”¾å…¥ `data/corpus/raw/` å’Œ `raw/important_examples/`

### ç¬¬å››æ­¥ï¼šä¸€é”®æ™ºèƒ½å¾®è°ƒ

```powershell
# åœ¨ PowerShell æˆ– CMD ä¸­è¿è¡Œ
python scripts/smart_finetune.py
```

æˆ–è€…ä½¿ç”¨ make (éœ€å®‰è£… [GNU Make for Windows](http://gnuwin32.sourceforge.net/packages/make.htm)):
```powershell
make finetune-smart
```

---

## ç¡¬ä»¶é…ç½®æ–‡ä»¶ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰

ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹ä½ çš„ç¡¬ä»¶å¹¶é€‰æ‹©æœ€ä½³è®­ç»ƒå‚æ•°ï¼š

| ç¡¬ä»¶ | å†…å­˜ | batch_size | num_layers | iters |
|------|------|------------|------------|-------|
| M1/M2/M3 8GB | 8GB | 1 | 4 | 300 |
| M1/M2/M3 16GB | 16GB | 2 | 8 | 500 |
| M1/M2/M3 Max 32GB+ | 32GB+ | 4 | 16 | 1000 |
| M1/M2/M3 Ultra 64GB+ | 64GB+ | 8 | 32 | 1500 |

**è‡ªå®šä¹‰é…ç½®ï¼š** ç¼–è¾‘ `config/training_profiles.json` æ–‡ä»¶ã€‚

**æŸ¥çœ‹ç³»ç»Ÿæ£€æµ‹ç»“æœï¼š**
```bash
python scripts/finetune_mlx_mac.py --auto --check-only
```

---

## æ‰‹åŠ¨é…ç½®æƒé‡ï¼ˆé«˜çº§ï¼‰

å¦‚æœä½ æƒ³ç²¾ç¡®æ§åˆ¶æ¯ç¯‡æ–‡ç« çš„æƒé‡ï¼Œå¯ä»¥ç¼–è¾‘ `data/corpus/priority_weights.json`ï¼š

```json
{
  "default_weight": 1.0,
  "priority_folder_weight": 2.5,

  "priority_docs": {
    "Barka_MicrobiolMolBiolRev2016": {
      "weight": 3.0,
      "reason": "æœ€èƒ½ä»£è¡¨ Gilles é£æ ¼çš„ç»¼è¿°"
    }
  },

  "exclude_docs": {
    "some_bad_paper": {
      "reason": "å¤ªçŸ­ï¼Œä¸èƒ½ä»£è¡¨é£æ ¼"
    }
  }
}
```

**æŸ¥çœ‹æ‰€æœ‰æ–‡ç«  IDï¼š**
```bash
make list-docs
```

---

## å®Œæ•´ Makefile å‘½ä»¤

```bash
# === è¯­æ–™ç®¡ç† ===
make corpus            # æŸ¥çœ‹è¯­æ–™åº“çŠ¶æ€
make corpus-guide      # æ˜¾ç¤ºæ·»åŠ æ–‡ä»¶æŒ‡å—
make corpus-validate   # éªŒè¯æ‰€æœ‰è¯­æ–™æ–‡ä»¶
make parse-corpus      # è§£æ raw/ ä¸­çš„æ–‡ç« 
make list-docs         # åˆ—å‡ºæ‰€æœ‰æ–‡ç«  ID
make training-stats    # æŸ¥çœ‹è®­ç»ƒæ•°æ®ç»Ÿè®¡

# === æ™ºèƒ½è®­ç»ƒ ===
make finetune-smart    # ğŸš€ ä¸€é”®æ™ºèƒ½è®­ç»ƒï¼ˆè‡ªåŠ¨æ£€æµ‹å¹³å°å’Œç¡¬ä»¶ï¼‰
make finetune-all      # Mac ä¸€é”®è®­ç»ƒï¼ˆparse + prepare + mlxï¼‰

# === åˆ†æ­¥è®­ç»ƒ ===
make prepare-training  # ç”Ÿæˆè®­ç»ƒæ•°æ®
make finetune-mlx      # Mac MLX å¾®è°ƒ
make finetune-lora     # Linux/Windows LoRA å¾®è°ƒ

# === ç¯å¢ƒæ£€æŸ¥ ===
make check-mlx         # æ£€æŸ¥ MLX ä¾èµ– (Mac)
make check-lora        # æ£€æŸ¥ LoRA ä¾èµ– (Linux/Windows)
```

---

## å¦‚ä½•å‡å°‘ AI æ£€æµ‹ï¼Ÿ

å¾®è°ƒåçš„æ¨¡å‹ä¼šæ›´å¥½åœ°æ¨¡ä»¿äººç±»å†™ä½œé£æ ¼ï¼Œä½†è¿˜å¯ä»¥é‡‡å–ä»¥ä¸‹æªæ–½ï¼š

### 1. ä½¿ç”¨é«˜è´¨é‡è¯­æ–™

- æ”¾å…¥æ›´å¤š Gilles çš„æ–‡ç« ï¼ˆè¶Šå¤šè¶Šå¥½ï¼‰
- æŠŠæœ€èƒ½ä»£è¡¨é£æ ¼çš„æ”¾å…¥ `important_examples/`
- æ’é™¤ä¸å…¸å‹çš„æ–‡ç« 

### 2. è°ƒæ•´ç”Ÿæˆå‚æ•°

åœ¨ `.env` ä¸­è®¾ç½®ï¼š
```bash
TEMPERATURE_BASE=0.4      # ç•¥é«˜çš„æ¸©åº¦å¢åŠ å˜åŒ–
TEMPERATURE_VARIANCE=0.2  # å˜ä½“é—´æ›´å¤§å·®å¼‚
```

### 3. åå¤„ç†

- è½»å¾®ç¼–è¾‘ç”Ÿæˆçš„æ–‡æœ¬
- æ·»åŠ ä¸ªäººè¡¨è¾¾
- è°ƒæ•´å¥å­ç»“æ„

---

## DPO è¿›é˜¶è®­ç»ƒï¼ˆåå¥½å¯¹é½ï¼‰

ä½¿ç”¨åï¼Œä½ å¯ä»¥é€šè¿‡åé¦ˆè¿›ä¸€æ­¥ä¼˜åŒ–ï¼š

1. ä½¿ç”¨ GSWA ç”Ÿæˆå˜ä½“
2. åœ¨ UI ä¸­ä¸ºå˜ä½“è¯„åˆ†ï¼ˆBest/Good/Badï¼‰
3. æäº¤åé¦ˆ
4. å¯¼å‡ºå¹¶è®­ç»ƒï¼š

```bash
make export-dpo
python scripts/prepare_training_data.py --format dpo --from-feedback
make finetune-lora
```

---

## æ•…éšœæ’é™¤

### Q: æ²¡æœ‰æ£€æµ‹åˆ°æ–‡ç« ï¼Ÿ

A: æ£€æŸ¥æ–‡ä»¶ä½ç½®å’Œæ ¼å¼ï¼š
```bash
ls data/corpus/raw/
ls data/corpus/raw/important_examples/
```
ç¡®ä¿æ˜¯ `.pdf`, `.docx`, æˆ– `.txt` æ–‡ä»¶ã€‚

### Q: MLX è®­ç»ƒå¤ªæ…¢ï¼Ÿ

A: å‡å°‘è¿­ä»£æ¬¡æ•°æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼š
```bash
python scripts/finetune_mlx_mac.py --model phi --iters 500
```

### Q: å†…å­˜ä¸è¶³ (Mac)?

A: ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹ä½ çš„ç¡¬ä»¶å¹¶é€‰æ‹©åˆé€‚çš„é…ç½®ã€‚å¦‚éœ€æ‰‹åŠ¨è°ƒæ•´ï¼š
```bash
# æŸ¥çœ‹å¯ç”¨é…ç½®
python scripts/finetune_mlx_mac.py --list-profiles

# ä½¿ç”¨ä¿å®ˆé…ç½®ï¼ˆæœ€ä½å†…å­˜ï¼‰
python scripts/finetune_mlx_mac.py --profile conservative

# æˆ–è€…æ‰‹åŠ¨è®¾ç½®å‚æ•°
python scripts/finetune_mlx_mac.py --batch-size 1 --num-layers 4 --max-seq-length 512
```

### Q: æ˜¾å­˜ä¸è¶³ (CUDA OOM)ï¼Ÿ

A: ä½¿ç”¨ 4-bit é‡åŒ–ï¼š
```bash
python scripts/finetune_lora.py --quantize 4bit --batch-size 2
```

### Q: ç”Ÿæˆè´¨é‡ä¸‹é™ï¼Ÿ

A: å¯èƒ½æ˜¯è¿‡æ‹Ÿåˆï¼Œå°è¯•ï¼š
- å‡å°‘è®­ç»ƒè½®æ•°
- å¢åŠ æ›´å¤šæ–‡ç« 
- ä½¿ç”¨éªŒè¯é›†

### Q: å¦‚ä½•å›æ»šåˆ°åŸæ¨¡å‹ï¼Ÿ

A: ä¿®æ”¹ `.env`ï¼š
```bash
VLLM_MODEL_NAME=mistral  # ä½¿ç”¨åŸå§‹æ¨¡å‹
```

---

## å‚è€ƒèµ„æº

- [LoRA è®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [QLoRA è®ºæ–‡](https://arxiv.org/abs/2305.14314)
- [DPO è®ºæ–‡](https://arxiv.org/abs/2305.18290)
- [MLX æ–‡æ¡£](https://ml-explore.github.io/mlx/)
- [Ollama æ¨¡å‹åˆ›å»º](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)
